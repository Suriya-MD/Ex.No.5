

# EXP 5: COMPARATIVE ANALYSIS OF DIFFERENT TYPES OF PROMPTING PATTERNS AND EXPLAIN WITH VARIOUS TEST SCENARIOS
### Name: SURIYA M
### Reg No: 212223110055
# Aim: To test and compare how different pattern models respond to various prompts (broad or unstructured) versus basic prompts (clearer and more refined) across multiple scenarios.  Analyze the quality, accuracy, and depth of the generated responses 

### AI Tools Required: 

# Explanation: 
Define the Two Prompt Types:

Write a basic Prompt: Clear, detailed, and structured prompts that give specific instructions or context to guide the model.
Based on that pattern type refined the prompt and submit that with AI tool.
Get the ouput and write the report.

Prepare Multiple Test Scenarios:
Select various scenarios such as:
Generating a creative story.
Answering a factual question.
Summarizing an article or concept.
Providing advice or recommendations.
Or Any other test scenario
For each scenario, create both a naïve and a basic prompt. Ensure each pair of prompts targets the same task but with different levels of structure.
Run Experiments with ChatGPT:
Input the naïve prompt for each scenario and record the generated response.
Then input the corresponding basic prompt and capture that response.
Repeat this process for all selected scenarios to gather a full set of results.
Evaluate Responses : 
	Compare how ChatGPT performs when given naïve versus basic prompts and analyze the output based on Quality,Accuracy and Depth. Also analyse does ChatGPT consistently provide better results with basic prompts? Are there scenarios where naïve prompts work equally well?
Deliverables:
A table comparing ChatGPT's responses to naïve and basic prompts across all scenarios.
Analysis of how prompt clarity impacts the quality, accuracy, and depth of ChatGPT’s outputs.
Summary of findings with insights on how to structure prompts for optimal results when using ChatGPT.

# OUTPUT

**Experiment Results**

**Scenario 1: Creative Story**

• Naïve Prompt: "Write a story about a robot."

• Basic Prompt: "Write a 300-word creative story about a robot who learns human

emotions while living in a futuristic city. Include a twist at the end.

<img width="793" height="330" alt="image" src="https://github.com/user-attachments/assets/48d57199-47c3-41cb-9f51-cb629b42a01b" />

**Scenario 2: Factual Question**


• Naïve Prompt: "What is AI?"

• Basic Prompt: "Explain Artificial Intelligence in 150 words, highlighting its

definition, main applications, and one advantage and disadvantage."

<img width="796" height="309" alt="image" src="https://github.com/user-attachments/assets/f6749471-49e4-4bff-ba80-91d320b5ebe2" />

**Scenario 3: Summarization**

• Naïve Prompt: "Summarize climate change."

• Basic Prompt: "Summarize the concept of climate change in 5 bullet points,

covering its causes, effects, and solutions.

<img width="771" height="232" alt="image" src="https://github.com/user-attachments/assets/53823aad-6e46-42ba-9621-faf3c73ba45d" />

**Scenario 4: Advice / Recommendation**

• Naïve Prompt: "Give me advice about studying."

• Basic Prompt: "Give 5 practical study tips for college students preparing for final

exams, focusing on time management, note-taking, and avoiding distractions."

<img width="793" height="289" alt="image" src="https://github.com/user-attachments/assets/80522e3b-775a-4b32-a235-08ca03d66c90" />


**Overall Analysis**

• Quality: Basic prompts consistently produced clearer, structured, and contextrich answers.

• Accuracy: Naïve prompts often resulted in incomplete or vague answers, while

basic prompts provided targeted, reliable information.

• Depth: Naïve prompts gave surface-level responses. Basic prompts enabled

detailed, nuanced, and creative outputs.

• Observation: In very simple factual queries (e.g., “What is AI?”), naïve prompts

worked reasonably well, but still lacked depth. For creative, summarization, or

advice tasks, structured prompts performed far better.

<img width="767" height="557" alt="image" src="https://github.com/user-attachments/assets/02e2e528-3207-40da-948c-539a42690e1c" />


**Summary of Findings**

1. Prompt clarity strongly impacts ChatGPT outputs.

o Naïve prompts → vague, shallow, and generic responses.

o Basic prompts → structured, detailed, and accurate responses.

2. Best Practices for Prompting:

o Be specific about task (e.g., “5 tips,” “150 words”).

o Provide context (e.g., “for college students,” “in futuristic city”).

o Request formatting (e.g., bullet points, word limit).


**Conclusion:**

This experiment confirms that ChatGPT consistently provides better results with

basic, well-structured prompts. Naïve prompts, while a quick way to get an initial idea,

lead to generic, often shallow, and less useful outputs.

There are no scenarios in this test where the naïve prompts worked equally well. Even for

the simple factual question, the basic prompt's output was superior due to its organized

structure and richer detail. The key takeaway is that the more specific and detailed the

instructions given to the AI, the better the output will be. Users can optimize their results

by:

• **Defining the Goal**: Clearly state what you want the AI to achieve.

• **Setting the Persona**: Ask the AI to adopt a specific role or persona (e.g., "a travel

agent," "a technical writer").

• **Providing Constraints**: Specify the length, format (e.g., bullet points,

paragraphs), and tone.

• **Including Contex**t: Add relevant background information to help the AI

understand the request more deeply.

<img width="651" height="460" alt="image" src="https://github.com/user-attachments/assets/52da7260-6a9c-44dc-a7b7-1f02c36330a8" />


# RESULT:

The prompt for the above said problem executed successfully
